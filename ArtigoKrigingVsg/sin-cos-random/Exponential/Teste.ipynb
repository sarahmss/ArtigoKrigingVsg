{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T19:04:34.010790Z",
     "iopub.status.busy": "2024-09-09T19:04:34.010495Z",
     "iopub.status.idle": "2024-09-09T19:04:34.683581Z",
     "shell.execute_reply": "2024-09-09T19:04:34.682800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10480/3502052833.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../VirtualSamples.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m df_training \u001b[38;5;241m=\u001b[39m create_random_dataframe(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)   \n\u001b[1;32m     40\u001b[0m df_test \u001b[38;5;241m=\u001b[39m create_random_dataframe(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)       \n\u001b[0;32m---> 42\u001b[0m df_virtual \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../VirtualSamples.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExponential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/excel/_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/excel/_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../VirtualSamples.xlsx'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Função Func para calcular o valor da função nas coordenadas\n",
    "def Func(x, y):\n",
    "    z =  y*np.sin(x) - x*np.cos(y)\n",
    "    return z\n",
    "\n",
    "# Gera dados aleatórios ao invés de um grid regular\n",
    "def GetRandomSamples(n_samples):\n",
    "    # Gera n_samples pontos aleatórios no intervalo [-2, 2]\n",
    "    x = np.random.uniform(-5, 5, n_samples)\n",
    "    y = np.random.uniform(-5, 5, n_samples)\n",
    "    z = Func(x, y)\n",
    "    return x, y, z\n",
    "\n",
    "# Cria um DataFrame a partir dos pontos aleatórios\n",
    "def create_random_dataframe(n_samples):\n",
    "    x, y, z = GetRandomSamples(n_samples)\n",
    "    data = {'x1': x, 'x2': y, 'f(x1,x2)': z}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def GetGrid(grid):\n",
    "    x = np.linspace(-5, 5, grid)\n",
    "    y = np.linspace(-5, 5, grid)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = Func(x, y)\n",
    "    return x, y, z\n",
    "\n",
    "def create_dataframe(grid):\n",
    "   x, y, z = GetGrid(grid)\n",
    "   data = {'x1': x.flatten(), 'x2': y.flatten(), 'f(x1,x2)': z.flatten()}\n",
    "   df = pd.DataFrame(data)\n",
    "   return df\n",
    "\n",
    "\n",
    "df_1000 = create_dataframe(32)\n",
    "df_training = create_random_dataframe(n_samples=25)   \n",
    "df_test = create_random_dataframe(n_samples=16)       \n",
    "\n",
    "df_virtual = pd.read_excel(\"../VirtualSamples.xlsx\", sheet_name=\"Exponential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T19:04:34.745193Z",
     "iopub.status.busy": "2024-09-09T19:04:34.744709Z",
     "iopub.status.idle": "2024-09-09T19:04:36.174929Z",
     "shell.execute_reply": "2024-09-09T19:04:36.174294Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = StandardScaler()\n",
    "out_scaler = StandardScaler()\n",
    "\n",
    "def show_norm(df, label=\"data\", plot=False):\n",
    "    df_norm = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "    df_denorm = pd.DataFrame(scaler.inverse_transform(df_norm), columns=df_norm.columns)\n",
    "\n",
    "    if (plot):\n",
    "        df.plot(title=f\"{label}: Original data\")\n",
    "        df_norm.plot(title=f\"{label}: Normalized data\")\n",
    "        df_denorm.plot(title=f\"{label}: Denormalized data\")\n",
    "    return (df_norm)\n",
    "\n",
    "\n",
    "def test_out_scaler(df):\n",
    "    out = df[\"f(x1,x2)\"].values.reshape(-1, 1)  \n",
    "    plt.plot(out, label='Original')\n",
    "    out_scaler.fit(out)\n",
    "    norm = out_scaler.transform(out)\n",
    "    plt.plot(norm, label='Normalizado')\n",
    "    plt.plot(out_scaler.inverse_transform(norm), label='desnormalizado')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "scaler.fit(df_training)\n",
    "test_out_scaler(df_training)\n",
    "\n",
    "df_training_norm = show_norm(df_training, \"Training\")\n",
    "df_1000_norm = show_norm(df_1000)\n",
    "df_test_norm = show_norm(df_test)\n",
    "df_virtual_norm = show_norm(pd.concat([df_training, df_virtual]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T19:04:36.178008Z",
     "iopub.status.busy": "2024-09-09T19:04:36.177709Z",
     "iopub.status.idle": "2024-09-09T19:04:36.696284Z",
     "shell.execute_reply": "2024-09-09T19:04:36.695227Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"./content\")\n",
    "\n",
    "lm_dir = \"tf-levenberg-marquardt\"\n",
    "if not os.path.exists(lm_dir):\n",
    "  !git clone https://github.com/fabiodimarco/$lm_dir\n",
    "\n",
    "os.chdir(lm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T19:04:36.703164Z",
     "iopub.status.busy": "2024-09-09T19:04:36.701297Z",
     "iopub.status.idle": "2024-09-09T19:04:36.709375Z",
     "shell.execute_reply": "2024-09-09T19:04:36.708550Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    _input = np.vstack([df['x1'], df['x2']]).T\n",
    "    _output = np.array(df['f(x1,x2)'])\n",
    "    return (_input, _output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T19:04:36.715674Z",
     "iopub.status.busy": "2024-09-09T19:04:36.714121Z",
     "iopub.status.idle": "2024-09-09T19:04:39.368283Z",
     "shell.execute_reply": "2024-09-09T19:04:39.367551Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "import levenberg_marquardt as lm\n",
    "\n",
    "# layers, neurons\n",
    "class ShuffleArchitecture:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, act_h, act_o, param_reg):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.act_h = act_h\n",
    "        self.act_o = act_o\n",
    "        self.regularizer = regularizers.L2(param_reg)\n",
    "        self.initializer = initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=np.random.randint(1, 10000))\n",
    "\n",
    "    def compute_k(self):\n",
    "        total_parameters = 0\n",
    "        for layer in self.model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) > 0:  \n",
    "                for w in weights:\n",
    "                    total_parameters += np.prod(w.shape)\n",
    "        return total_parameters\n",
    "        \n",
    "    def set_architecture(self):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(self.hidden_sizes[0],\n",
    "                        input_shape=(self.input_size,),\n",
    "                        activation=self.act_h,\n",
    "                        kernel_regularizer=self.regularizer,\n",
    "                        kernel_initializer=self.initializer,                        \n",
    "                        ))  # input layer\n",
    "\n",
    "        for size in self.hidden_sizes[1:]:  # hidden layers\n",
    "            self.model.add(tf.keras.layers.Dense(size,\n",
    "                            activation=self.act_h,\n",
    "                            kernel_regularizer=self.regularizer,\n",
    "                            kernel_initializer=self.initializer,  \n",
    "                        ))\n",
    "\n",
    "        self.model.add(tf.keras.layers.Dense(self.output_size,\n",
    "                        activation=self.act_o,\n",
    "                        kernel_regularizer=self.regularizer,\n",
    "                        kernel_initializer=self.initializer,  \n",
    "                        ))  # output layer\n",
    "\n",
    "    def create_model(self, _learning_rate):\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=_learning_rate),\n",
    "            loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "        self.lm_model = lm.ModelWrapper(\n",
    "            tf.keras.models.clone_model(self.model))\n",
    "\n",
    "        self.lm_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=_learning_rate),\n",
    "            loss=lm.MeanSquaredError())\n",
    "        return(self.lm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T19:04:39.371704Z",
     "iopub.status.busy": "2024-09-09T19:04:39.371125Z",
     "iopub.status.idle": "2024-09-09T19:04:39.428703Z",
     "shell.execute_reply": "2024-09-09T19:04:39.428086Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error, mean_absolute_percentage_error \n",
    "\n",
    "class TrainWithSmallDataset:\n",
    "    def __init__(self, batch_size=1000):\n",
    "        self.batch_size = batch_size\n",
    "        self.betters = []\n",
    "        self.k = 0\n",
    "\n",
    "    def create_dataset(self, input, output):\n",
    "      input = tf.expand_dims(tf.cast(input, tf.float32), axis=-1)\n",
    "      output = tf.expand_dims(tf.cast(output, tf.float32), axis=-1)\n",
    "      dataset = tf.data.Dataset.from_tensor_slices((input, output))\n",
    "      dataset = dataset.shuffle(len(input))\n",
    "      dataset = dataset.batch(self.batch_size).cache()\n",
    "      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "      return (dataset, input, output)\n",
    "\n",
    "    def split_dataset(self, tester):\n",
    "      input_train, input_val, output_train, output_val = train_test_split(tester.input, tester.output, test_size=0.2, shuffle = True)\n",
    "\n",
    "      self.dataset, self.input, self.output = self.create_dataset(tester.input, tester.output)\n",
    "      self.train_dataset, self.train_input, self.train_output = self.create_dataset(input_train, output_train)\n",
    "      self.test_dataset, self.test_input, self.test_output = self.create_dataset(tester.input_test, tester.output_test)\n",
    "      self.val_dataset, self.val_input, self.val_output = self.create_dataset(input_val, output_val)\n",
    "      self.sup_dataset, self.sup_input, self.sup_output = self.create_dataset(tester.input_1000, tester.output_1000)\n",
    "\n",
    "      self._train = (input_train, output_train)\n",
    "      self._val = (input_val, output_val)\n",
    "      self._test = (tester.input_test, tester.output_test)\n",
    "      self._data = (tester.input, tester.output)\n",
    "      self._sup = (tester.input_1000, tester.output_1000)\n",
    "\n",
    "\n",
    "    def train_using_lm(self, train_dataset, epochs=1000):\n",
    "      early_stopping_monitor = EarlyStopping(monitor='val_loss',\n",
    "                                              patience=6,\n",
    "                                              restore_best_weights=True)\n",
    "      \n",
    "      self.lm_model.save_weights('./initial-.weights.h5')\n",
    "      self.results = self.lm_model.fit(train_dataset,\n",
    "                                            epochs=epochs,\n",
    "                                            validation_data=self.val_dataset,\n",
    "                                            callbacks=[early_stopping_monitor],\n",
    "                                            verbose=0)\n",
    "      print (\"Stopped at epoch: \", early_stopping_monitor.stopped_epoch)\n",
    "    \n",
    "    def get_new_metrics(self, orig, pred, r2, mse):\n",
    "      n = len(orig) # N: quantidade de saidas\n",
    "      k = self.k\n",
    "      waste = (orig.flatten() - pred.flatten())\n",
    "\n",
    "      mape = mean_absolute_percentage_error(orig, pred)  \n",
    "      r2_adj = 1 - (((n - 1)/(n - k - 1)) * (1 - r2))\n",
    "      rsd = np.sqrt(np.sum(waste ** 2) / (n - 2))\n",
    "      rmse = root_mean_squared_error(orig, pred)          \n",
    "      aic = (-2 * np.log(mse)) + (2 * k)\n",
    "      bic = (-2 * np.log(mse)) + (k * np.log(n))\n",
    "      return (mape, r2_adj, rsd, rmse, aic, bic)\n",
    "      \n",
    "\n",
    "    def get_metrics(self):\n",
    "          # Calculando a saida com os dados normalizados\n",
    "          pred = self.lm_model.predict(self.input).flatten()\n",
    "          test_pred = self.lm_model.predict(self.test_input).flatten()\n",
    "          val_pred = self.lm_model.predict(self.val_input).flatten()\n",
    "          sup_pred = self.lm_model.predict(self.sup_input).flatten()\n",
    "\n",
    "          # Calculando as metricas com a saida desnormalizada\n",
    "          pred_denorm = out_scaler.inverse_transform(pred.reshape(-1, 1))\n",
    "          test_pred_denorm = out_scaler.inverse_transform(test_pred.reshape(-1, 1))\n",
    "          val_pred_denorm = out_scaler.inverse_transform(val_pred.reshape(-1, 1))\n",
    "          sup_pred_denorm = out_scaler.inverse_transform(sup_pred.reshape(-1, 1))\n",
    "\n",
    "          out_denorm = out_scaler.inverse_transform(self._data[1].reshape(-1, 1))\n",
    "          test_denorm = out_scaler.inverse_transform(self._test[1].reshape(-1, 1))\n",
    "          val_denorm = out_scaler.inverse_transform(self._val[1].reshape(-1, 1))\n",
    "          sup_denorm = out_scaler.inverse_transform(self._sup[1].reshape(-1, 1))\n",
    "\n",
    "          r2 = r2_score(out_denorm, pred_denorm)\n",
    "          r2_test = r2_score(test_denorm, test_pred_denorm)\n",
    "          r2_val = r2_score(val_denorm, val_pred_denorm)\n",
    "          r2_sup = r2_score(sup_denorm,  sup_pred_denorm)\n",
    "\n",
    "          mse = mean_squared_error(out_denorm, pred_denorm)\n",
    "          mse_test = mean_squared_error(test_denorm, test_pred_denorm)\n",
    "          mse_val = mean_squared_error(val_denorm, val_pred_denorm)\n",
    "          mse_sup = mean_squared_error(sup_denorm,  sup_pred_denorm)\n",
    "          \n",
    "          mape, r2_adj, rsd, rmse, aic, bic = self.get_new_metrics(out_denorm, pred_denorm, r2, mse)\n",
    "          metrics = {\n",
    "                          'r2': r2,\n",
    "                          'r2_sup': r2_sup,\n",
    "                          'r2_test': r2_test,\n",
    "                          'r2_val': r2_val,\n",
    "                          'mse': mse,\n",
    "                          'mse_sup': mse_sup,\n",
    "                          'mse_test': mse_test,\n",
    "                          'mse_val': mse_val,\n",
    "                          'mape': mape,\n",
    "                          'rmse': rmse,\n",
    "                          'r2_adj': r2_adj,\n",
    "                          'rsd': rsd,\n",
    "                          'aic': aic,\n",
    "                          'bic': bic\n",
    "                          }\n",
    "\n",
    "          return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T19:04:39.432017Z",
     "iopub.status.busy": "2024-09-09T19:04:39.431652Z",
     "iopub.status.idle": "2024-09-09T19:04:39.450574Z",
     "shell.execute_reply": "2024-09-09T19:04:39.449708Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from itertools import product\n",
    "import shutil\n",
    "\n",
    "class Tester:\n",
    "  def __init__(self, _df, _df_1000, _df_test,  run_times=500, dataset_run_times=10):\n",
    "    self.run_times = run_times\n",
    "    self.better_metrics = {}\n",
    "    self.dataset_run_times = dataset_run_times\n",
    "    self.input, self.output = split_df(_df)\n",
    "    self.input_1000, self.output_1000 = split_df(_df_1000)\n",
    "    self.input_test, self.output_test = split_df(_df_test)\n",
    "  \n",
    "  def setArchitecure(self, trainer, _hidden_sizes, _pg, _lr):\n",
    "    shuffler = ShuffleArchitecture(input_size=2,\n",
    "                                    hidden_sizes=_hidden_sizes,\n",
    "                                    output_size=1,\n",
    "                                    act_h='tanh',\n",
    "                                    act_o='linear',\n",
    "                                    param_reg=_pg)\n",
    "    shuffler.set_architecture()    \n",
    "    trainer.lm_model = shuffler.create_model(_lr)\n",
    "    trainer.k = shuffler.compute_k()\n",
    "\n",
    "  def Train(self, trainer, epochs=1000):\n",
    "    trainer.train_using_lm(trainer.train_dataset, epochs=epochs)\n",
    "    return(trainer.get_metrics(), trainer.lm_model)\n",
    "\n",
    "  def SaveModelWeights(self, model, fileName):\n",
    "    path = f\"../models/{fileName}.keras\"\n",
    "    open(path,'w').close()\n",
    "    model.save_weights(path)\n",
    "    shutil.move(\"./initial-.weights.h5\",\n",
    "                f\"../models/initial-weights/{fileName}.keras\")\n",
    "\n",
    "  def SaveDataset(self, trainer, fileName):\n",
    "    path = f\"../dataset/{fileName}.pkl\" \n",
    "    with open(path, 'wb') as f:\n",
    "      pickle.dump((trainer._data, trainer._train, trainer._val, trainer._test), f)\n",
    "      \n",
    "  def LoopWeights(self, sort_by, boundarie, trainer, idx):\n",
    "    better_model = 0\n",
    "    save = False\n",
    "\n",
    "    for i in range(self.run_times):\n",
    "      print (f\"+++++++++++ [{idx}] | {i + 1} ++++++++++++++++++\")\n",
    "      metrics, model = self.Train(trainer)\n",
    "      if (metrics[sort_by] <= boundarie): # should be >= to acsending metrics\n",
    "        fileName = f\"model_{idx}_{better_model}\"\n",
    "        self.SaveModelWeights(model, fileName)\n",
    "        self.better_metrics[fileName] = metrics\n",
    "        better_model += 1\n",
    "        save = True\n",
    "    return(save)\n",
    "\n",
    "  def Loop(self, sort_by, boundarie, hidden_sizes, regularizers, learning_rate):\n",
    "    trainer = TrainWithSmallDataset()\n",
    "\n",
    "    for count, (hidden_size, reg, lr) in enumerate(product(hidden_sizes, regularizers, learning_rate), start=1):\n",
    "      header =  f\"Hidden Size={hidden_size}, regularizer={reg}, learning_rate={lr}\"\n",
    "      print(f\"Testando combinacao{count}: {header}\")\n",
    "      self.setArchitecure(trainer, hidden_size, reg, lr)\n",
    "      for j in range(self.dataset_run_times):\n",
    "        trainer.split_dataset(self)\n",
    "        if (self.LoopWeights(sort_by, boundarie, trainer, f\"{count}_{j}\") == True):\n",
    "          self.SaveDataset(trainer, f\"dataset_{count}_{j}\")\n",
    "          self.DisplayBetterResults('mse_sup', header, f\"{count}_{j}\")\n",
    "        self.better_metrics = {}\n",
    "\n",
    "  def DisplayBetterResults(self, sort_by, header, dataset=0):\n",
    "    df = pd.DataFrame.from_dict(self.better_metrics, orient='index')\n",
    "    df = df.sort_values([sort_by])\n",
    "    display(df)\n",
    "    path = f'../results/metrics_{dataset}'\n",
    "    df.to_excel(f\"{path}.xlsx\", index=True)\n",
    "    print(f\"DataFrame salvo em {path}\")\n",
    "    with open(f\"{path}.txt\", 'w') as arquivo:\n",
    "      arquivo.write(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando apenas com dados originais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T19:04:39.454737Z",
     "iopub.status.busy": "2024-09-09T19:04:39.454279Z",
     "iopub.status.idle": "2024-09-09T19:10:17.695909Z",
     "shell.execute_reply": "2024-09-09T19:10:17.695082Z"
    }
   },
   "outputs": [],
   "source": [
    "tester = Tester(\n",
    "                _df=df_virtual_norm,\n",
    "                _df_1000=df_1000_norm,\n",
    "                _df_test = df_test_norm,\n",
    "                run_times=10, dataset_run_times=50)\n",
    "tester.Loop(sort_by='mse',\n",
    "            boundarie = 0.5,\n",
    "            hidden_sizes = [[4], [8], [12], [24], [36],\n",
    "                            [4, 2], [8, 4], [12, 6], [24, 12], [36, 18]],\n",
    "            regularizers=[0.02],\n",
    "            learning_rate=[0.01])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
